{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CROPLAND AI Christmas Card Generator\n",
        "The code in this notebook clones the public GitHub repository of our 2022 Christmas Card generator application. In this repository, we store the data and code for an application that lets end users generate a personalized Christmas Card using generative AI. Next, we how how you can finetune GPT-2 to create a particular genre of text, namely, Christmas/New Year's wishes. \n",
        "\n",
        "As this is a fairly large model, you will want to run this notebook on a hardware-accelerated runtime. "
      ],
      "metadata": {
        "id": "evH4LyAftbQN"
      },
      "id": "evH4LyAftbQN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone the git repository to fetch the data "
      ],
      "metadata": {
        "id": "6E1lmZzak6nc"
      },
      "id": "6E1lmZzak6nc"
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash \n",
        "git clone https://github.com/cropland-bv/ai_christmas_card_app"
      ],
      "metadata": {
        "id": "004zaqVwkjiD"
      },
      "id": "004zaqVwkjiD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install missing Python dependencies"
      ],
      "metadata": {
        "id": "QQh30F-uma5B"
      },
      "id": "QQh30F-uma5B"
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers fastai"
      ],
      "metadata": {
        "id": "p_eJ9LKLmaPk"
      },
      "id": "p_eJ9LKLmaPk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run imports"
      ],
      "metadata": {
        "id": "mg1v0-o4icR3"
      },
      "id": "mg1v0-o4icR3"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "b570b48b",
      "metadata": {
        "id": "b570b48b"
      },
      "outputs": [],
      "source": [
        "import os, random, glob\n",
        "import numpy as np\n",
        "from fastai.text.all import *\n",
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
        "from transformers import TextDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments,AutoModelWithLMHead\n",
        "from transformers import TextGenerationPipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define data paths and fetch tokenizer from HuggingFace hub"
      ],
      "metadata": {
        "id": "KwsHHOi5i2LI"
      },
      "id": "KwsHHOi5i2LI"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "d099b746",
      "metadata": {
        "id": "d099b746"
      },
      "outputs": [],
      "source": [
        "path = os.path.join(os.getcwd(), \"ai_christmas_card_app\",\"training_gpt2\", \"input\", \"poemsdataset\", \"forms\", \"nieuwjaarsbrieven\")\n",
        "files = glob.glob( path +'/*.txt' )\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
        "train_path = 'train.txt'\n",
        "test_path = 'test.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create train/test split based on the list of txt files downloaded from GitHub\n",
        "We write 80% of the files to \"train.txt\", and reserve 20% of the files for the test set."
      ],
      "metadata": {
        "id": "43zBsol-oTNZ"
      },
      "id": "43zBsol-oTNZ"
    },
    {
      "cell_type": "code",
      "source": [
        "train = open(\"train.txt\", \"a\")\n",
        "test = open(\"test.txt\", \"a\")\n",
        "train_indices = random.sample(range(len(files)), int(0.8*len(files)))\n",
        "i=0\n",
        "while i < len(files):\n",
        "  if(i in train_indices):\n",
        "    with open(files[i], \"r\") as text_input:\n",
        "      train.write(text_input.read())\n",
        "      text_input.close()\n",
        "  else:\n",
        "     with open(files[i], \"r\") as text_input:\n",
        "      test.write(text_input.read())\n",
        "      text_input.close()\n",
        "  i = i +1\n",
        "    \n",
        "train.close()\n",
        "test.close()\n"
      ],
      "metadata": {
        "id": "AWGf6KvDoafC"
      },
      "id": "AWGf6KvDoafC",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions to load and tokenize the dataset"
      ],
      "metadata": {
        "id": "-sOVKdMinqeo"
      },
      "id": "-sOVKdMinqeo"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_dataset(file_path, tokenizer, block_size = 75):\n",
        "    dataset = TextDataset(\n",
        "        tokenizer = tokenizer,\n",
        "        file_path = file_path,\n",
        "        block_size = block_size,\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "def load_data_collator(tokenizer, mlm = False):\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer, \n",
        "        mlm=mlm,\n",
        "    )\n",
        "    return data_collator\n",
        "\n"
      ],
      "metadata": {
        "id": "9y0LdJGmnprk"
      },
      "id": "9y0LdJGmnprk",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply loading/tokenization functions to create a train and a test dataset for our model fine-tuning"
      ],
      "metadata": {
        "id": "ITH-Qp-_nuQE"
      },
      "id": "ITH-Qp-_nuQE"
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = load_dataset('train.txt', tokenizer)\n",
        "test_dataset = load_dataset('test.txt', tokenizer)\n",
        "data_collator = load_data_collator(tokenizer)"
      ],
      "metadata": {
        "id": "8KrzgPp1ntzE"
      },
      "id": "8KrzgPp1ntzE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pull the GPT-2 medium model checkpoint from the HuggingFace hub"
      ],
      "metadata": {
        "id": "dVH4bl1dr9Ut"
      },
      "id": "dVH4bl1dr9Ut"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9955f4b",
      "metadata": {
        "scrolled": true,
        "id": "b9955f4b"
      },
      "outputs": [],
      "source": [
        "model = AutoModelWithLMHead.from_pretrained(\"gpt2-medium\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the arguments for fine-tuning the model"
      ],
      "metadata": {
        "id": "7kWfi8w0s9Ey"
      },
      "id": "7kWfi8w0s9Ey"
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir= os.path.join(os.getcwd(), \"app\", \"model\",\"files_for_huggingface_2\"), #The output directory\n",
        "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
        "    num_train_epochs=3, # number of training epochs\n",
        "    per_device_train_batch_size=2, # batch size for training\n",
        "    per_device_eval_batch_size=2,  # batch size for evaluation\n",
        "    eval_steps = 400, # Number of update steps between two evaluations.\n",
        "    save_steps=800, # after # steps model is saved \n",
        "    warmup_steps=500,# number of warmup steps for learning rate scheduler\n",
        "    learning_rate = 9.120108734350652e-05,\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "LV36P1nts0X-"
      },
      "id": "LV36P1nts0X-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creater a trainer object to perform the fine-tuning"
      ],
      "metadata": {
        "id": "ohCJMtJBtDR8"
      },
      "id": "ohCJMtJBtDR8"
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "tlWhi1IGs2Mh"
      },
      "id": "tlWhi1IGs2Mh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the train() method to fine-tune the model\n",
        "At this point, you may want to stretch your legs for a bit, because it will take some time"
      ],
      "metadata": {
        "id": "wU_AYrEKtLb5"
      },
      "id": "wU_AYrEKtLb5"
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "c6Ba_FvSs3LW"
      },
      "id": "c6Ba_FvSs3LW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## After training, save the model"
      ],
      "metadata": {
        "id": "NJ4sJevztTOd"
      },
      "id": "NJ4sJevztTOd"
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "5_Z-N-dQs4xh"
      },
      "id": "5_Z-N-dQs4xh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reload the saved model for inference"
      ],
      "metadata": {
        "id": "SoihMshksWRk"
      },
      "id": "SoihMshksWRk"
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = GPT2LMHeadModel.from_pretrained(os.path.join(os.getcwd(), \"app\", \"model\",\"files_for_huggingface_2\"), max_length=250, min_length = 150, num_beams=5, no_repeat_ngram_size=2, early_stopping=True, temperature = 1.5)\n",
        "TOKENIZER = AutoTokenizer.from_pretrained('gpt2-medium')\n",
        "PIPE =   TextGenerationPipeline(model=MODEL, tokenizer=TOKENIZER, return_all_scores=True, skip_special_tokens=True)\n",
        "TOKENIZER.save_pretrained(os.path.join(os.getcwd(), \"app\", \"model\",\"files_for_huggingface_2\"))\n",
        "\n",
        "output= PIPE('With a new year comes')[0]['generated_text']\n",
        "print(output)"
      ],
      "metadata": {
        "id": "pXxZ0H3usTzg"
      },
      "id": "pXxZ0H3usTzg",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tensorflow",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "e947b190f444fe9e0219cbb7c18033652bfd237c5832724645e7af33ebabdd86"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}